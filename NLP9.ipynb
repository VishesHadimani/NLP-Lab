{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 1.2927\n",
      "Epoch 100 Loss 0.0258\n",
      "Epoch 200 Loss 0.0081\n",
      "Epoch 300 Loss 0.0045\n",
      "Epoch 400 Loss 0.0031\n",
      "Epoch 500 Loss 0.0023\n",
      "Epoch 600 Loss 0.0018\n",
      "Epoch 700 Loss 0.0015\n",
      "Epoch 800 Loss 0.0013\n",
      "Epoch 900 Loss 0.0011\n",
      "hello -> bonjour\n",
      "world -> \n",
      "machine -> machine\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define vocabulary and translation pairs\n",
    "english_words = ['hello', 'world', 'machine', 'learning', 'deep', 'neural', 'network', 'translate', 'language', 'model']\n",
    "french_words = ['bonjour', 'monde', 'machine', 'apprentissage', 'profond', 'neuronal', 'réseau', 'traduire', 'langue', 'modèle']\n",
    "\n",
    "# Create word to index mappings\n",
    "en_word2idx = {word: idx for idx, word in enumerate(english_words)}\n",
    "fr_word2idx = {word: idx for idx, word in enumerate(french_words)}\n",
    "en_idx2word = {idx: word for word, idx in en_word2idx.items()}\n",
    "fr_idx2word = {idx: word for word, idx in fr_word2idx.items()}\n",
    "\n",
    "# Encoder-Decoder Model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "# Training the Model\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "    decoder_input = torch.tensor([[0]])  # Start of sentence token\n",
    "    decoder_hidden = encoder_hidden\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == 1:  # End of sentence token\n",
    "            break\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item() / target_length\n",
    "\n",
    "# Prepare training data\n",
    "def tensor_from_sentence(sentence, word2idx):\n",
    "    indexes = [word2idx[word] for word in sentence.split()]\n",
    "    indexes.append(1)  # End of sentence token\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 256\n",
    "learning_rate = 0.01\n",
    "n_epochs = 1000\n",
    "\n",
    "# Initialize models, optimizers, and loss function\n",
    "encoder = Encoder(len(english_words), hidden_size)\n",
    "decoder = Decoder(hidden_size, len(french_words))\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for en_word, fr_word in zip(english_words, french_words):\n",
    "        input_tensor = tensor_from_sentence(en_word, en_word2idx)\n",
    "        target_tensor = tensor_from_sentence(fr_word, fr_word2idx)\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch} Loss {loss:.4f}')\n",
    "\n",
    "# Translate a word\n",
    "def translate(word):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensor_from_sentence(word, en_word2idx)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        decoder_input = torch.tensor([[0]])  # Start of sentence token\n",
    "        decoder_hidden = encoder_hidden\n",
    "        translated_word = ''\n",
    "        for di in range(10):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == 1:\n",
    "                break\n",
    "            translated_word += fr_idx2word[topi.item()] + ' '\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        return translated_word.strip()\n",
    "\n",
    "# Test the translation\n",
    "test_words = ['hello', 'world', 'machine']\n",
    "for word in test_words:\n",
    "    translated = translate(word)\n",
    "    print(f'{word} -> {translated}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
